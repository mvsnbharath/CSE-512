{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech classification**\n",
    "\n",
    "In this section we will attempt at POS classification, using SVMs, a one-vs-all (OVA) framework, and some word embeddings as features.\n",
    "\n",
    "Deliver from this section\n",
    "\n",
    "* loss, train and test misclassification  plots for noun-vs-all, for primal projected subgradient method\n",
    "* comparison of loss functions for primal and dual linear SVM\n",
    "* evidence of cross validation to determine best value of $C$ in the primal linear SVM and $C$ and $\\sigma$ in Kernel SVM\n",
    "* confusion matrix of final answer on train, validation, and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "Load the data you saved in the previous part. Scramble and split the data into a train, validation, and test set, by 60/40/40 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-693d759a7c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word_pos_data.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_emb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sio' is not defined"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat('word_pos_data.mat')\n",
    "X = data['word_emb']\n",
    "y = data['word_label'][0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verb-vs-all, primal projected subgradient descent**\n",
    "\n",
    "Create a binary classification task that determines whether a word is a noun or not. Run the primal projected subgradient descent method  on this task, and report the loss. \n",
    "\n",
    "Note that this will benefit from some precomputation for the projection operator, outside of the loop.\n",
    "\n",
    "Use a step size scheme of $\\alpha_k = 1/k$, and run for 100 iterations. Determine the best value of $C$ using some form of cross validation.\n",
    "\n",
    "Report also the following performance metrics for the train, validation, and test sets:\n",
    " * the misclassification rate\n",
    " * the number of false positives\n",
    " * the number of false negatives\n",
    " \n",
    "Using these observations, argue for a best choice of $C$ via some form of cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noun-vs-all, dual projected gradient descent**\n",
    "\n",
    "Run the dual projected gradient descent method on this task (no kernels yet), and report the loss  and the train/test misclassification rate. Use a constant step size of 1/L where L is the smoothness parameter of your objective function. Use the value of C that you determined was best in the previous section. Verify that you get basically the same answers for the same choice of $C$.\n",
    "\n",
    "Submit a plot showing the loss function of the dual vs that of the primal, plotted in a way that verifies the two optimal values match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel SVM and One-vs-all**\n",
    "\n",
    "Finally, code up a dual form Kernel SVM, and run it on the same binary classification task. Using this last classifier, run a one-vs-all framework to achieve multiclass classification on all the different classes. You may use your validation set to tune the following:\n",
    "\n",
    "* early stopping point\n",
    "* different thresholds, used to alleviate class imbalance\n",
    "* metric used to pick the previous: misclassification rate, or F1 score?\n",
    "\n",
    "Submit your final confusion matrices of the train, validation, and test set on the best possible predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
